{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/18187558737/ResNet8_CIFAR10_PGD_2/blob/main/ResNet18_CIFAR10_PGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUYp8V5kZ4LA",
        "outputId": "5b8f63fb-3ed1-4e06-e738-1d512cbc25fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # 授权 Google Drive\n",
        "sys.path.append('/content/drive/MyDrive/320') # 此为你的自定义模块的存储路径"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Yy8DCWFAXpLX"
      },
      "outputs": [],
      "source": [
        "\n",
        "from ResNet import *\n",
        "\n",
        "from load_local_cifar10 import *\n",
        "from easydict import EasyDict\n",
        "from projected_gradient_descent import projected_gradient_descent\n",
        "from fast_gradient_method import fast_gradient_method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FPJQbkp5ZPzW"
      },
      "outputs": [],
      "source": [
        "cifar10_dir = '/content/drive/MyDrive/cifar-10-batches-py'\n",
        "(x_train, y_train), (x_test, y_test) = load_data(cifar10_dir)\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S66iR8pcZXOW",
        "outputId": "c003d2ae-3977-48f8-d5eb-57bd0085156f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "391/391 [==============================] - 101s 250ms/step - loss: 1.3747 - sparse_categorical_accuracy: 0.5649 - val_loss: 1.5253 - val_sparse_categorical_accuracy: 0.5150\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 97s 248ms/step - loss: 0.8217 - sparse_categorical_accuracy: 0.7432 - val_loss: 1.1024 - val_sparse_categorical_accuracy: 0.6696\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 97s 248ms/step - loss: 0.6067 - sparse_categorical_accuracy: 0.8085 - val_loss: 0.8059 - val_sparse_categorical_accuracy: 0.7446\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 96s 246ms/step - loss: 0.4599 - sparse_categorical_accuracy: 0.8538 - val_loss: 0.8571 - val_sparse_categorical_accuracy: 0.7237\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 97s 249ms/step - loss: 0.3573 - sparse_categorical_accuracy: 0.8867 - val_loss: 0.6861 - val_sparse_categorical_accuracy: 0.7879\n",
            "Model: \"res_net18_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_20 (Conv2D)          multiple                  1728      \n",
            "                                                                 \n",
            " batch_normalization_20 (Bat  multiple                 256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_17 (Activation)  multiple                  0         \n",
            "                                                                 \n",
            " sequential_1 (Sequential)   (None, 4, 4, 512)         11176448  \n",
            "                                                                 \n",
            " global_average_pooling2d_1   multiple                 0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,183,562\n",
            "Trainable params: 11,173,962\n",
            "Non-trainable params: 9,600\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = ResNet18([2, 2, 2, 2])\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['sparse_categorical_accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "checkpoint_save_path = \"/content/drive/MyDrive/320/ResNet50.ckpt\"\n",
        "if os.path.exists(checkpoint_save_path + '.index'):\n",
        "    print('-------------load the model-----------------')\n",
        "    model.load_weights(checkpoint_save_path)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_save_path,\n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True)\n",
        "\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=5, validation_data=(x_test, y_test), validation_freq=1,\n",
        "                    callbacks=[cp_callback])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CYJY1yVRZawd"
      },
      "outputs": [],
      "source": [
        "train_loss = tf.metrics.Mean(name=\"train_loss\")\n",
        "test_acc_clean = tf.metrics.SparseCategoricalAccuracy()\n",
        "test_acc_fgsm = tf.metrics.SparseCategoricalAccuracy()\n",
        "test_acc_pgd = tf.metrics.SparseCategoricalAccuracy()\n",
        "optimizer = tf.optimizers.Adam()\n",
        "loss_fun = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "train_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "nb_epochs = 1\n",
        "eps = 0.05\n",
        "adv_train = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NS-iy4HCZe3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64f3f7d2-e3fa-4893-ed69-44673f5ab886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000/50000 [=====>........................] - ETA: 6:31:17 - loss: 2.1570"
          ]
        }
      ],
      "source": [
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(x)\n",
        "        loss = loss_fun(y, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    train_loss(loss)\n",
        "    train_metric.update_state(y, predictions)\n",
        "\n",
        "# Train model with adversarial training\n",
        "for epoch in range(nb_epochs):\n",
        "    # keras like display of progress\n",
        "    progress_bar_train = tf.keras.utils.Progbar(50000)\n",
        "    for (x, y) in zip(x_test, y_test):\n",
        "        x = tf.cast(x, dtype=tf.float32)\n",
        "        if adv_train:\n",
        "            # Replace clean example with adversarial example for adversarial training\n",
        "            x = tf.expand_dims(x, 0)\n",
        "            x = projected_gradient_descent(model, x, eps, 0.01, 40, np.inf)\n",
        "        train_step(x, y)\n",
        "        progress_bar_train.add(x.shape[0], values=[(\"loss\", train_loss.result())])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8jPxaALxZjvA"
      },
      "outputs": [],
      "source": [
        "cifar10_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "cifar10_test = tf.data.Dataset.from_tensor_slices((x_test[0:1000], y_test[0:1000]))\n",
        "data = EasyDict(train=cifar10_train, test=cifar10_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mTzhbBjqZnyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32e38ede-d7e8-4ab7-dada-b4912f500a48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000/1000 [==============================] - 862s 862ms/step\n",
            "test acc on clean examples (%): 36.965\n",
            "test acc on FGM adversarial examples (%): 27.764\n",
            "test acc on PGD adversarial examples (%): 26.575\n"
          ]
        }
      ],
      "source": [
        "progress_bar_test = tf.keras.utils.Progbar(1000)\n",
        "\n",
        "for x, y in cifar10_test:\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    y_pred = model(x)\n",
        "    test_acc_clean(y, y_pred)\n",
        "\n",
        "    x_fgm = fast_gradient_method(model, x, eps, np.inf)\n",
        "    y_pred_fgm = model(x_fgm)\n",
        "    test_acc_fgsm(y, y_pred_fgm)\n",
        "\n",
        "    x_pgd = projected_gradient_descent(model, x, eps, 0.01, 40, np.inf)\n",
        "    y_pred_pgd = model(x_pgd)\n",
        "    test_acc_pgd(y, y_pred_pgd)\n",
        "\n",
        "    progress_bar_test.add(x.shape[0])\n",
        "  \n",
        "print(\n",
        "    \"test acc on clean examples (%): {:.3f}\".format(test_acc_clean.result() * 100)\n",
        ")                                                                                                             \n",
        "print(\n",
        "    \"test acc on FGM adversarial examples (%): {:.3f}\".format(\n",
        "        test_acc_fgsm.result() * 100\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"test acc on PGD adversarial examples (%): {:.3f}\".format(\n",
        "        test_acc_pgd.result() * 100\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CIFAR - 10\n",
        "\n",
        "# To decode the files\n",
        "import pickle\n",
        "# For array manipulations\n",
        "import numpy as np\n",
        "# To make one-hot vectors\n",
        "from tensorflow.python.keras.utils import np_utils\n",
        "# To plot graphs and display images\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# constants\n",
        "\n",
        "path = \"/content/drive/MyDrive/cifar-10-batches-py/\"  # Path to data\n",
        "\n",
        "# Height or width of the images (32 x 32)\n",
        "size = 32\n",
        "\n",
        "# 3 channels: Red, Green, Blue (RGB)\n",
        "channels = 3\n",
        "\n",
        "# Number of classes\n",
        "num_classes = 10\n",
        "\n",
        "# Each file contains 10000 images\n",
        "image_batch = 10000\n",
        "\n",
        "# 5 training files\n",
        "num_files_train = 5\n",
        "\n",
        "# Total number of training images\n",
        "images_train = image_batch * num_files_train\n",
        "\n",
        "\n",
        "# https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "\n",
        "def unpickle(file):\n",
        "    # Convert byte stream to object\n",
        "    with open(path + file, 'rb') as fo:\n",
        "        print(\"Decoding file: %s\" % (path + file))\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "\n",
        "    # Dictionary with images and labels\n",
        "    return dict\n",
        "\n",
        "\n",
        "def convert_images(raw_images):\n",
        "    # Convert images to numpy arrays\n",
        "\n",
        "    # Convert raw images to numpy array and normalize it\n",
        "    raw = np.array(raw_images, dtype=float) / 255.0\n",
        "\n",
        "    # Reshape to 4-dimensions - [image_number, channel, height, width]\n",
        "    images = raw.reshape([-1, channels, size, size])\n",
        "\n",
        "    images = images.transpose([0, 2, 3, 1])\n",
        "\n",
        "    # 4D array - [image_number, height, width, channel]\n",
        "    return images\n",
        "\n",
        "\n",
        "def load_data(file):\n",
        "    # Load file, unpickle it and return images with their labels\n",
        "\n",
        "    data = unpickle(file)\n",
        "\n",
        "    # Get raw images\n",
        "    images_array = data[b'data']\n",
        "\n",
        "    # Convert image\n",
        "    images = convert_images(images_array)\n",
        "    # Convert class number to numpy array\n",
        "    labels = np.array(data[b'labels'])\n",
        "\n",
        "    # Images and labels in np array form\n",
        "    return images, labels\n",
        "\n",
        "\n",
        "def get_test_data():\n",
        "    # Load all test data\n",
        "\n",
        "    images, labels = load_data(file=\"test_batch\")\n",
        "\n",
        "    # Images, their labels and\n",
        "    # corresponding one-hot vectors in form of np arrays\n",
        "    return images, labels, np_utils.to_categorical(labels, num_classes)\n",
        "\n",
        "\n",
        "def get_train_data():\n",
        "    # Load all training data in 5 files\n",
        "\n",
        "    # Pre-allocate arrays\n",
        "    images = np.zeros(shape=[images_train, size, size, channels], dtype=float)\n",
        "    labels = np.zeros(shape=[images_train], dtype=int)\n",
        "\n",
        "    # Starting index of training dataset\n",
        "    start = 0\n",
        "\n",
        "    # For all 5 files\n",
        "    for i in range(num_files_train):\n",
        "        # Load images and labels\n",
        "        images_batch, labels_batch = load_data(file=\"data_batch_\" + str(i + 1))\n",
        "\n",
        "        # Calculate end index for current batch\n",
        "        end = start + image_batch\n",
        "\n",
        "        # Store data to corresponding arrays\n",
        "        images[start:end, :] = images_batch\n",
        "        labels[start:end] = labels_batch\n",
        "\n",
        "        # Update starting index of next batch\n",
        "        start = end\n",
        "\n",
        "    # Images, their labels and\n",
        "    # corresponding one-hot vectors in form of np arrays\n",
        "    return images, labels, np_utils.to_categorical(labels, num_classes)\n",
        "\n",
        "\n",
        "def get_class_names():\n",
        "    # Load class names\n",
        "    raw = unpickle(\"batches.meta\")[b'label_names']\n",
        "\n",
        "    # Convert from binary strings\n",
        "    names = [x.decode('utf-8') for x in raw]\n",
        "\n",
        "    # Class names\n",
        "    return names\n",
        "\n",
        "\n",
        "def plot_images(images, labels_true, class_names, labels_pred=None):\n",
        "    assert len(images) == len(labels_true)\n",
        "\n",
        "    # Create a figure with sub-plots\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
        "\n",
        "    # Adjust the vertical spacing\n",
        "    if labels_pred is None:\n",
        "        hspace = 0.2\n",
        "    else:\n",
        "        hspace = 0.5\n",
        "    fig.subplots_adjust(hspace=hspace, wspace=0.3)\n",
        "\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        # Fix crash when less than 9 images\n",
        "        if i < len(images):\n",
        "            # Plot the image\n",
        "            ax.imshow(images[i], interpolation='spline16')\n",
        "\n",
        "            # Name of the true class\n",
        "            labels_true_name = class_names[labels_true[i]]\n",
        "\n",
        "            # Show true and predicted classes\n",
        "            if labels_pred is None:\n",
        "                xlabel = \"True: \" + labels_true_name\n",
        "            else:\n",
        "                # Name of the predicted class\n",
        "                labels_pred_name = class_names[labels_pred[i]]\n",
        "\n",
        "                xlabel = \"True: \" + labels_true_name + \"\\nPredicted: \" + labels_pred_name\n",
        "\n",
        "            # Show the class on the x-axis\n",
        "            ax.set_xlabel(xlabel)\n",
        "\n",
        "        # Remove ticks from the plot\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_model(model_details):\n",
        "    # Create sub-plots\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Summarize history for accuracy\n",
        "    axs[0].plot(range(1, len(model_details.history['acc']) + 1), model_details.history['acc'])\n",
        "    axs[0].plot(range(1, len(model_details.history['val_acc']) + 1), model_details.history['val_acc'])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1, len(model_details.history['acc']) + 1), len(model_details.history['acc']) / 10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "\n",
        "    # Summarize history for loss\n",
        "    axs[1].plot(range(1, len(model_details.history['loss']) + 1), model_details.history['loss'])\n",
        "    axs[1].plot(range(1, len(model_details.history['val_loss']) + 1), model_details.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1, len(model_details.history['loss']) + 1), len(model_details.history['loss']) / 10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualize_errors(images_test, labels_test, class_names, labels_pred, correct):\n",
        "    incorrect = (correct == False)\n",
        "\n",
        "    # Images of the test-set that have been incorrectly classified.\n",
        "    images_error = images_test[incorrect]\n",
        "\n",
        "    # Get predicted classes for those images\n",
        "    labels_error = labels_pred[incorrect]\n",
        "\n",
        "    # Get true classes for those images\n",
        "    labels_true = labels_test[incorrect]\n",
        "\n",
        "    # Plot the first 9 images.\n",
        "    plot_images(images=images_error[0:9],\n",
        "                labels_true=labels_true[0:9],\n",
        "                class_names=class_names,\n",
        "                labels_pred=labels_error[0:9])\n",
        "\n",
        "\n",
        "def predict_classes(model, images_test, labels_test):\n",
        "    # Predict class of image using model\n",
        "    class_pred = model.predict(images_test, batch_size=32)\n",
        "\n",
        "    # Convert vector to a label\n",
        "    labels_pred = np.argmax(class_pred, axis=1)\n",
        "\n",
        "    # Boolean array that tell if predicted label is the true label\n",
        "    correct = (labels_pred == labels_test)\n",
        "\n",
        "    # Array which tells if the prediction is correct or not\n",
        "    # And predicted labels\n",
        "    return correct, labels_pred\n",
        "\n"
      ],
      "metadata": {
        "id": "lvun87whElLa"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = get_class_names()\n",
        "print(class_names)\n",
        "\n",
        "num_classes = len(class_names)\n",
        "print(num_classes)\n",
        "\n",
        "# Hight and width of the images\n",
        "IMAGE_SIZE = 32\n",
        "# 3 channels, Red, Green and Blue\n",
        "CHANNELS = 3\n",
        "\n",
        "images_train, labels_train, class_train = get_train_data()\n",
        "print(labels_train)\n",
        "\n",
        "print(class_train)\n",
        "\n",
        "images_test, labels_test, class_test = get_test_data()\n",
        "print(\"Training set size:\\t\", len(images_train))\n",
        "print(\"Testing set size:\\t\", len(images_test))\n",
        "\n",
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "\n",
        "y_pred = model.predict(images_test, batch_size=32)\n",
        "print(y_pred[0])\n",
        "\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "print(y_pred)\n",
        "\n",
        "correct = (y_pred == labels_test)\n",
        "print(correct)\n",
        "\n",
        "\n",
        "print(\"Number of correct predictions: %d\" % sum(correct))\n",
        "\n",
        "num_images = len(correct)\n",
        "print(\"Accuracy: %.2f%%\" % ((sum(correct)*100)/num_images))\n",
        "\n",
        "print(\"Number of incorrect predictions: %d\" % (num_images-sum(correct)))\n",
        "incorrect = (correct == False)\n",
        "\n",
        "# Images of the test-set that have been incorrectly classified.\n",
        "images_error = x_test[incorrect]\n",
        "\n",
        "# Get predicted classes for those images\n",
        "labels_error = y_pred[incorrect]\n",
        "\n",
        "# Get true classes for those images\n",
        "labels_true = y_test[incorrect]\n",
        "num_error_class = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "for x in labels_true:\n",
        "  if x == 0:\n",
        "    num_error_class[0] += 1\n",
        "  if x == 1:\n",
        "    num_error_class[1] += 1\n",
        "  if x == 2:\n",
        "    num_error_class[2] += 1\n",
        "  if x == 3:\n",
        "    num_error_class[3] += 1\n",
        "  if x == 4:\n",
        "    num_error_class[4] += 1\n",
        "  if x == 5:\n",
        "    num_error_class[5] += 1\n",
        "  if x == 6:\n",
        "    num_error_class[6] += 1\n",
        "  if x == 7:\n",
        "    num_error_class[7] += 1\n",
        "  if x == 8:\n",
        "    num_error_class[8] += 1\n",
        "  if x == 9:\n",
        "    num_error_class[9] += 1\n",
        "print(num_error_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "golLHbeqBBdh",
        "outputId": "8e8c5366-9d9a-4040-b844-ba297ad66c52"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoding file: /content/drive/MyDrive/cifar-10-batches-py/batches.meta\n",
            "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
            "10\n",
            "Decoding file: /content/drive/MyDrive/cifar-10-batches-py/data_batch_1\n",
            "Decoding file: /content/drive/MyDrive/cifar-10-batches-py/data_batch_2\n",
            "Decoding file: /content/drive/MyDrive/cifar-10-batches-py/data_batch_3\n",
            "Decoding file: /content/drive/MyDrive/cifar-10-batches-py/data_batch_4\n",
            "Decoding file: /content/drive/MyDrive/cifar-10-batches-py/data_batch_5\n",
            "[6 9 9 ... 9 1 1]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]]\n",
            "Decoding file: /content/drive/MyDrive/cifar-10-batches-py/test_batch\n",
            "Training set size:\t 50000\n",
            "Testing set size:\t 10000\n",
            "Accuracy: 33.59%\n",
            "[0.06569295 0.00763711 0.19293246 0.19513622 0.13268217 0.17651458\n",
            " 0.05859552 0.13485917 0.03180804 0.00414169]\n",
            "[3 8 8 ... 5 5 7]\n",
            "[ True  True  True ...  True False  True]\n",
            "Number of correct predictions: 3359\n",
            "Accuracy: 33.59%\n",
            "Number of incorrect predictions: 6641\n",
            "[441, 692, 645, 957, 688, 577, 896, 624, 452, 669]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ResNet18_CIFAR10_PGD.ipynb",
      "provenance": [],
      "mount_file_id": "1nZHcXp4iwh2jU3jO5s54Rt5oK6nrg5WA",
      "authorship_tag": "ABX9TyPOlDJ7OTeEDGk+abr63d8R",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}